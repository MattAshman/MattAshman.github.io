<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Talks on Matthew Ashman</title>
    <link>https://MattAshman.github.io/talks/</link>
    <description>Recent content in Talks on Matthew Ashman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Feb 2021 08:33:00 +0000</lastBuildDate><atom:link href="https://MattAshman.github.io/talks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inference in Stochastic Processes @ MLG</title>
      <link>https://MattAshman.github.io/talks/mlg_stochasticprocesses/</link>
      <pubDate>Thu, 25 Feb 2021 08:33:00 +0000</pubDate>
      
      <guid>https://MattAshman.github.io/talks/mlg_stochasticprocesses/</guid>
      <description>[slides] [video]
In parametric models, probabilistic inference is most often approached by computing a posterior distribution over model weights. These weights are then marginalised to obtain a distribution over functions and make predictions. If our goal is solely to make good predictions, an appealing alternative is to directly perform inference over the &amp;lsquo;function-space&amp;rsquo; or predictive posterior distribution of our models, without considering the posterior distribution over the weights. Using Gaussian Processes (GPs) as motivation, this talk starts by introducing a method for constructing more general stochastic processes based on combining basis functions with random weights.</description>
    </item>
    
    <item>
      <title>Variational Bayes as Surrogate Regression @ MLG</title>
      <link>https://MattAshman.github.io/talks/mlg_vbsurrogateregression/</link>
      <pubDate>Wed, 10 Feb 2021 13:54:00 +0000</pubDate>
      
      <guid>https://MattAshman.github.io/talks/mlg_vbsurrogateregression/</guid>
      <description>[slides]
Variational Bayes is a useful approximate inference framework in which an intractable posterior distribution is approximated by simpler tractable one. The extent to which this is useful (usually) depends on how closely this approximation matches reality, and how quickly it can be obtained. We&amp;rsquo;ll present lines of work that utilise the posteriors of tractable models as this approximation, and the interesting inference algorithms that arise in this setting.
Although we&amp;rsquo;ll cover all of these in the presentation, it will be helpful to have some familiarity with the basics of variational Bayes (e.</description>
    </item>
    
  </channel>
</rss>
