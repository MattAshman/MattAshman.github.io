<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>paper on Matthew Ashman</title>
    <link>https://MattAshman.github.io/categories/paper/</link>
    <description>Recent content in paper on Matthew Ashman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Feb 2021 08:37:00 +0000</lastBuildDate><atom:link href="https://MattAshman.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scalable Gaussian Process Variational Autoencoders</title>
      <link>https://MattAshman.github.io/projects/scalablegpvae/</link>
      <pubDate>Thu, 25 Feb 2021 08:37:00 +0000</pubDate>
      
      <guid>https://MattAshman.github.io/projects/scalablegpvae/</guid>
      <description>Collaboration with Metod Jazbec, Vincent Fortuin, Michael Pearce, Stephen Mandt and Gunnar RÃ¤tsch.
[paper]
Conventional variational autoencoders fail in modeling correlations between data points due to their use of factorized priors. Amortized Gaussian process inference through GPVAEs has led to significant improvements in this regard, but is still inhibited by the intrinsic complexity of exact GP inference. We improve the scalability of these methods through principled sparse inference approaches. We propose a new scalable GPVAE model that outperforms existing approaches in terms of runtime and memory footprint, is easy to implement, and allows for joint end-to-end optimization of all components.</description>
    </item>
    
    <item>
      <title>Sparse Gaussian Process Variational Autoencoders</title>
      <link>https://MattAshman.github.io/projects/sgpvae/</link>
      <pubDate>Fri, 02 Oct 2020 22:55:05 -0400</pubDate>
      
      <guid>https://MattAshman.github.io/projects/sgpvae/</guid>
      <description>Collaboration with Jonathan So, Will Tebbutt, Vincent Fortuin, Michael Pearce and Professor Richard E. Turner.
[paper]
Large, multi-dimensional spatio-temporal datasets are omnipresent in modern science and engineering. An effective framework for handling such data are Gaussian process deep generative models (GP-DGMs), which employ GP priors over the latent variables of DGMs. Existing approaches for performing inference in GP-DGMs do not support sparse GP approximations based on inducing points, which are essential for the computational efficiency of GPs, nor do they handle missing data &amp;ndash; a natural occurrence in many spatio-temporal datasets &amp;ndash; in a principled manner.</description>
    </item>
    
  </channel>
</rss>
